{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InsideSherpaJPMC_module2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrbMmla_NBFp",
        "colab_type": "text"
      },
      "source": [
        "# JPMC InsideSherpa - Module 2 \n",
        "Pedro Teixeira - O734271\n",
        "\n",
        "## Challenge\n",
        "\n",
        "> Input a name on the command prompt, match it with the above list and return a “Hit” or “No hit” based on a 75% match.\n",
        ">\n",
        "> ### Expected technical output:\n",
        ">\t\n",
        "> A Reusable Python/Java Classes which can used to Identify Payee is Matching against given list of Sanctions (List which contains Name of Individuals, Organizations and Countries provided as part of CSV file) and returns with Hit / No with Percentages,\n",
        ">\n",
        "> ### Expected user action/input\n",
        ">\n",
        "> Payee Information through command line argument or build a simple rest endpoint in Python/Java is real bonus\n",
        ">\n",
        "> ### Tools\n",
        ">\n",
        "> Python with Jupiter notebook setup or Java Implementation + Rest endpoint is real bonus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4upe2v8LN5Nt",
        "colab_type": "text"
      },
      "source": [
        "## Defining a match\n",
        "\n",
        "* What does a 75% match mean?\n",
        "\n",
        "It is not clear exactly what the task means by a 75% match. If we assume the simplest possible solution it would be using the hamming distance. However, the Hamming distance is not appropriate to compare string of different lengths.\n",
        "\n",
        "The US Department of the Treasury [seems to use Jaro-Winkler](https://www.treasury.gov/resource-center/sanctions/SDN-List/Pages/fuzzy_logic.aspx) distance/similarity measure. We will try to define something that we can implement, as Jaro-Winkler seems to be a little to complex, even though good libraries are available.\n",
        "\n",
        "After some research on string similarity quickly brings up lots of options: phonetically based algorithms, fuzzy logic, etc. (see https://stackabuse.com/levenshtein-distance-and-text-similarity-in-python/). \n",
        "\n",
        "The [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) seemed to be the most appropriate as it represents the minimum number of edits to transform one of the strings into the other. It is also not too complex and can be solved with dynammic programming. I would even claim that it goes well with the context of someone trying to escape the sanction screening, in the way that humans would think of how to modify our name. Phonetic algorithms would be the next step.\n",
        "\n",
        "### Properties of Levenshtein distance\n",
        "\n",
        "From the Wikipedia page:\n",
        "\n",
        "> The Levenshtein distance has several simple upper and lower bounds. These include:\n",
        "> * It is at least the difference of the sizes of the two strings.\n",
        "* **It is at most the length of the longer string.**\n",
        "* It is zero if and only if the strings are equal.\n",
        "* If the strings are the same size, the Hamming distance is an upper bound on the Levenshtein distance.\n",
        "* The Levenshtein distance between two strings is no greater than the sum of their Levenshtein distances from a third string (triangle inequality).\n",
        "\n",
        "[We can use this to derive a **basic** similarity score](https://stackoverflow.com/questions/6087281/similarity-score-levenshtein):\n",
        "\n",
        "`similarity = 1 - (levenshtein_distance / n)`  where n is the length of the longest string.\n",
        "\n",
        "Since the scores here tend to be pretty low, we can apply logarithmic scaling and define it as:\n",
        "\n",
        "`similarity = 1 / (1 - log(1 - (levenshtein_distance / n))) `\n",
        "\n",
        "or\n",
        "\n",
        "$ similarity = \\frac{1}{1-log(score))} $,\n",
        "\n",
        "where $ score = 1 - \\frac{levenshtein\\_distance}{n} $\n",
        "\n",
        "$log$ could be the logarithm of any basse in the above formulas. The base can be varied to adjust the desired scaling given appropriate data on how it is performing. Some quick testing reveals that base 20 seems to give similar scores to the US DoT tool, but further investigation would be needed to confirm this. Nevertheless, we will use base 20 for this exercise.\n",
        "\n",
        "This will be our definition of match for this task:\n",
        "\n",
        "> 2 strings match iff this logarithmic similarity measure is greater than the set threshold (75%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u-JObmWtXSo",
        "colab_type": "text"
      },
      "source": [
        "## Utility code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAQWwKrMvFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize data\n",
        "names = [\n",
        "         \"Kristopher Doe\",\n",
        "         \"Iceland\",\n",
        "         \"Real Arctic Line\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrfNtl4AtEpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "299712dc-511f-4d04-8342-1c0beda0e826"
      },
      "source": [
        "# Install Levenshtein distance library\n",
        "!pip install textdistance"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textdistance\n",
            "  Downloading https://files.pythonhosted.org/packages/35/71/87133323736b9b0180f600d477507318dae0abde613a54df33bfd0248614/textdistance-4.2.0-py3-none-any.whl\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ4bu6XfteCT",
        "colab_type": "text"
      },
      "source": [
        "## Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC5zS44ODaON",
        "colab_type": "text"
      },
      "source": [
        "### Building blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2ScP9atu8aG",
        "colab_type": "text"
      },
      "source": [
        "#### With library code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xw9evxp2rGXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5a3b74c0-c4f5-46de-8370-e65c37d915b5"
      },
      "source": [
        "from textdistance import levenshtein\n",
        "\n",
        "test1 = levenshtein(\"text\", \"test\")\n",
        "test2 = levenshtein(\"potato\", \"boat\")\n",
        "\n",
        "print(\"Similarity 'text'-'test': \", test1)\n",
        "print(\"Similarity 'potato'-'boat': \", test2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity 'text'-'test':  1\n",
            "Similarity 'potato'-'boat':  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ-BVKqSvADn",
        "colab_type": "text"
      },
      "source": [
        "#### Own implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y-yOm7bvUfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U0zp-idu_ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distance(str1: str, str2: str):\n",
        "  \"\"\"\n",
        "  Naive Implementation of Levenshtein Distance\n",
        "  \n",
        "  According to this video: \n",
        "  https://www.youtube.com/watch?time_continue=584&v=We3YDTzNXEk&feature=emb_logo\n",
        "  \"\"\"\n",
        "  \n",
        "  # ignore case\n",
        "  str1 = str1.lower()\n",
        "  str2 = str2.lower()\n",
        "\n",
        "  m = len(str1)\n",
        "  n = len(str2)\n",
        "  dists = np.zeros((m+1, n+1))\n",
        "  dists[0] = np.arange(n+1)\n",
        "  dists[:,0] = np.arange(m+1)\n",
        "\n",
        "  for i in range(1, m+1):\n",
        "    for j in range(1, n+1):\n",
        "      if str1[i-1] == str2[j-1]:\n",
        "        dists[i,j] = dists[i-1,j-1]\n",
        "      else:\n",
        "        add = dists[i, j-1]\n",
        "        delete = dists[i-1, j]\n",
        "        replace = dists[i-1, j-1]\n",
        "        \n",
        "        dists[i,j] = min(add, delete, replace) + 1\n",
        "      \n",
        "  return int(dists[m,n]), m, n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_060yOTumyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similarity(str1: str, str2: str):\n",
        "  \"\"\"\n",
        "  Computes the basic similarity score according to the above discussion\n",
        "  \"\"\"\n",
        "  d, l1, l2 = distance(str1, str2)\n",
        "  return 1 - (d / max(l1, l2))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1MfKzKCdzC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import log\n",
        "\n",
        "def log_similarity(str1: str, str2: str):\n",
        "  \"\"\"\n",
        "  Computes the logarithmic similarity score according to the above discussion\n",
        "  \"\"\"\n",
        "  d, l1, l2 = distance(str1, str2)\n",
        "  score = 1 - (d / max(l1, l2))\n",
        "\n",
        "  if score == 0:\n",
        "    return 0\n",
        "\n",
        "  return 1 / (1 - log(score, 20))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElvuG1T_7Cph",
        "colab_type": "text"
      },
      "source": [
        "#### Some basic test cases just o make sure things look correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qec4JG8Lw8BW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4be244c3-64af-4037-b58e-4846d68b898e"
      },
      "source": [
        "# Testing basic examples\n",
        "test1, _, _ = distance(\"test\", \"text\")\n",
        "test2, _, _ = distance(\"potato\", \"boat\")\n",
        "\n",
        "print(\"Distance 'text'-'test': \", test1)\n",
        "print(\"Distance 'potato'-'boat': \", test2)\n",
        "\n",
        "assert test1 == 1\n",
        "assert test2 == 3"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distance 'text'-'test':  1\n",
            "Distance 'potato'-'boat':  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZSn-_d7Cigf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = similarity(\"potato\", \"boat\")\n",
        "\n",
        "assert score == 0.5 # 3 / 6"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLtD_D7WDV6T",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YJQOsV2DoFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will require some workaround if the sanctions list does not fit in\n",
        "# memory but it should be easily solved by batch loading it and persisting the \n",
        "# results every few batches if needed\n",
        "\n",
        "def screen_name(search_name: str, sanctions_list: str, threshold=0.75, similarity_function=log_similarity):\n",
        "    \"\"\"\n",
        "    Screens the given name against the given list of sanctioned names,\n",
        "    returning a list of names together with their similarity scores if \n",
        "    those are at least as large as the threshold.    \n",
        "    \"\"\"\n",
        "    sanctioned_names = None\n",
        "    with open(sanctions_list, 'r') as f:\n",
        "        # need to remove newline char at end of each name\n",
        "        # there should be no empty lines in the input file\n",
        "        sanctioned_names = [x[:-1] for x in f.readlines()]\n",
        "\n",
        "    hits = []\n",
        "    for sanctioned_name in sanctioned_names:\n",
        "        score = similarity_function(sanctioned_name, search_name)\n",
        "        if score >= threshold:\n",
        "            hits.append((sanctioned_name, round(score * 100)))\n",
        "\n",
        "    # sort the list of hits before returning\n",
        "    hits.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return hits\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJJBYX27MdPE",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "To reveal the accuracy of our efforts, let us test them first on some basic examples and then on the real SDN (Specially Designated Nationals) dataset, downloaded from the US Department of the Treasury, available [here](https://www.treasury.gov/ofac/downloads/sdn.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2mRcO4KE4F1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9657725b-23cf-4dd9-f2df-efe2deb13543"
      },
      "source": [
        "# Basic test examples\n",
        "sanctions_list = \"/content/drive/My Drive/Colab Notebooks/sanctions.txt\" # Adjust this according to the environment\n",
        "search_name = \"Kristophre Toe\"\n",
        "\n",
        "screen_name(search_name, sanctions_list)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Kristopher Doe', 93)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7Slk7nm7g4z",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning up the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxjZYbXUMU0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnfgjyAvNKf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "99c55111-5bd8-4169-d897-b219fa09d074"
      },
      "source": [
        "# Clean up the data to just extract the names into a file\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/sdn.csv\", header=None, skipfooter=1)\n",
        "\n",
        "sdn_names = df[1].to_list()\n",
        "\n",
        "with open('sdn_names.txt', 'w') as f:\n",
        "  for name in sdn_names:\n",
        "    f.write(name + '\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4COGWdRS4d6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "0bf60b80-00bf-4dd1-9c25-82e9a77bd76f"
      },
      "source": [
        "sanctions_list = \"sdn_names.txt\"\n",
        "search_name = \"fly dargon\"\n",
        "\n",
        "screen_name(search_name, sanctions_list)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('FLYING DRAGON', 86),\n",
              " ('RI, Man Gon', 79),\n",
              " ('AL FURQAN', 77),\n",
              " ('FAN PARDAZAN', 77),\n",
              " ('GAYE, Haroun', 77),\n",
              " ('YARAN', 77),\n",
              " ('AZARGOUN', 77),\n",
              " ('FAXON', 77),\n",
              " ('PDVSA CARDON', 77),\n",
              " ('BALA, Haradin', 76),\n",
              " ('FADUL, Farhub', 76),\n",
              " ('FARS SARPANAH', 76)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqhWfRuwiRmz",
        "colab_type": "text"
      },
      "source": [
        "## REST API\n",
        "\n",
        "As Jupyter seems to be more appropriate for investigative work, the REST API is included as aseparate file."
      ]
    }
  ]
}